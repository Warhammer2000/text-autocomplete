{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Dict, Tuple, Sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_DIR = 'data'\n",
        "RAW1 = os.path.join(DATA_DIR, 'raw_dataset.csv')\n",
        "RAW2 = os.path.join(DATA_DIR, 'training.1600000.processed.noemoticon.csv')\n",
        "PROCESSED = os.path.join(DATA_DIR, 'dataset_processed.csv')\n",
        "TRAIN = os.path.join(DATA_DIR, 'train.csv')\n",
        "VAL = os.path.join(DATA_DIR, 'val.csv')\n",
        "TEST = os.path.join(DATA_DIR, 'test.csv')\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "MAX_VOCAB = 30000\n",
        "SEQ_LEN = 16\n",
        "BATCH_SIZE = 512\n",
        "LR = 1e-3\n",
        "EPOCHS = 3\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SPECIAL_TOKENS = ['<pad>', '<unk>', '<eos>']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source CSV: data\\training.1600000.processed.noemoticon.csv\n",
            "Processed rows: 1000\n",
            "Splits -> train: 800 val: 100 test: 100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1000, 800, 100, 100)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Загрузка/чистка и сплиты\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Поиск исходного CSV, если стандартные пути отсутствуют\n",
        "def find_source_csv() -> str | None:\n",
        "    candidates = []\n",
        "    for root, _, files in os.walk(DATA_DIR):\n",
        "        for fn in files:\n",
        "            if not fn.lower().endswith('.csv'):\n",
        "                continue\n",
        "            full = os.path.join(root, fn)\n",
        "            if os.path.basename(full) in {'dataset_processed.csv','train.csv','val.csv','test.csv'}:\n",
        "                continue\n",
        "            candidates.append((full, os.path.getsize(full)))\n",
        "    if not candidates:\n",
        "        return None\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[0][0]\n",
        "\n",
        "def nonempty_rows(path: str) -> int:\n",
        "    try:\n",
        "        dfh = pd.read_csv(path, nrows=5)\n",
        "        return int(dfh.shape[0])\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "src_path = None\n",
        "# Предпочитаем Kaggle-файл, если он есть\n",
        "if os.path.exists(RAW2):\n",
        "    src_path = RAW2\n",
        "elif os.path.exists(RAW1) and nonempty_rows(RAW1) > 0:\n",
        "    src_path = RAW1\n",
        "else:\n",
        "    src_path = find_source_csv()\n",
        "\n",
        "if src_path is None:\n",
        "    raise FileNotFoundError('Положите CSV с колонкой text в data/raw_dataset.csv или Kaggle CSV в data/training.1600000.processed.noemoticon.csv')\n",
        "\n",
        "print('Source CSV:', src_path)\n",
        "\n",
        "# Чтение исходного CSV\n",
        "if os.path.basename(src_path).lower().startswith('training.1600000.processed.noemoticon'):\n",
        "    df = pd.read_csv(src_path, encoding='latin-1', header=None)\n",
        "    df = df[[5]].rename(columns={5:'text'})\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(src_path)\n",
        "        if 'text' not in df.columns:\n",
        "            tmp = pd.read_csv(src_path, header=None, encoding='latin-1')\n",
        "            df = tmp[[5]].rename(columns={5:'text'})\n",
        "    except Exception:\n",
        "        tmp = pd.read_csv(src_path, header=None, encoding='latin-1')\n",
        "        df = tmp[[5]].rename(columns={5:'text'})\n",
        "\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', ' ', t)\n",
        "    t = re.sub(r'@[\\\\w_]+', ' ', t)\n",
        "    t = re.sub(r'\\\\s+', ' ', t).strip()\n",
        "    return t\n",
        "\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "df = df.dropna().drop_duplicates().reset_index(drop=True)\n",
        "mask = df['text'].astype(str).str.len() > 0\n",
        "df = df[mask].reset_index(drop=True)\n",
        "\n",
        "# Используем весь датасет без сабсэмплинга\n",
        "MAX_ROWS = None\n",
        "if MAX_ROWS:\n",
        "    df = df.iloc[:MAX_ROWS]\n",
        "\n",
        "\n",
        "df.to_csv(PROCESSED, index=False, encoding='utf-8')\n",
        "print('Processed rows:', len(df))\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "idx = rng.permutation(len(df))\n",
        "n = len(df)\n",
        "n_test = int(n * TEST_RATIO)\n",
        "n_val = int(n * VAL_RATIO)\n",
        "n_train = max(0, n - n_val - n_test)\n",
        "train_idx = idx[:n_train]\n",
        "val_idx = idx[n_train:n_train+n_val]\n",
        "test_idx = idx[n_train+n_val:]\n",
        "\n",
        "pd.DataFrame({'text': df.iloc[train_idx]['text']}).to_csv(TRAIN, index=False, encoding='utf-8')\n",
        "pd.DataFrame({'text': df.iloc[val_idx]['text']}).to_csv(VAL, index=False, encoding='utf-8')\n",
        "pd.DataFrame({'text': df.iloc[test_idx]['text']}).to_csv(TEST, index=False, encoding='utf-8')\n",
        "\n",
        "print('Splits ->', 'train:', len(train_idx), 'val:', len(val_idx), 'test:', len(test_idx))\n",
        "len(df), len(train_idx), len(val_idx), len(test_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vocab:   0%|          | 0/800 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vocab: 100%|██████████| 800/800 [00:00<00:00, 177096.28it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3655, ['<pad>', '<unk>', '<eos>', 'i', 'to', 'the', 'my', 'a', 'and', 'is'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def tokenize(s: str) -> List[str]:\n",
        "    return s.split()\n",
        "\n",
        "\n",
        "texts = pd.read_csv(TRAIN)['text'].astype(str).tolist()\n",
        "cnt = Counter()\n",
        "for t in tqdm(texts, desc=\"vocab\"):\n",
        "    cnt.update(tokenize(t))\n",
        "\n",
        "vocab = SPECIAL_TOKENS + [w for w,_ in cnt.most_common(MAX_VOCAB) if w not in SPECIAL_TOKENS]\n",
        "vocab = vocab[:MAX_VOCAB]\n",
        "stoi: Dict[str,int] = {w:i for i,w in enumerate(vocab)}\n",
        "itos: Dict[int,str] = {i:w for w,i in stoi.items()}\n",
        "PAD, UNK, EOS = stoi['<pad>'], stoi['<unk>'], stoi['<eos>']\n",
        "\n",
        "def encode(s: str, add_eos: bool=True) -> List[int]:\n",
        "    ids = [stoi.get(t, UNK) for t in tokenize(s)]\n",
        "    if add_eos:\n",
        "        ids.append(EOS)\n",
        "    return ids\n",
        "\n",
        "len(vocab), list(stoi)[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1856, 236)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, sequences: Sequence[List[int]], seq_len: int):\n",
        "        self.samples: List[Tuple[List[int], List[int]]] = []\n",
        "        for s in sequences:\n",
        "            if len(s) < 2: continue\n",
        "            for st in range(0, max(0, len(s)-1)):\n",
        "                ed = st + seq_len\n",
        "                x = s[st:ed]\n",
        "                y = s[st+1:ed+1]\n",
        "                if len(x) < seq_len or len(y) < seq_len: break\n",
        "                self.samples.append((x,y))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    return torch.stack(xs,0), torch.stack(ys,0)\n",
        "\n",
        "train_ids = [encode(s) for s in pd.read_csv(TRAIN)['text'].astype(str).tolist()]\n",
        "val_ids = [encode(s) for s in pd.read_csv(VAL)['text'].astype(str).tolist()]\n",
        "\n",
        "train_ds = NextTokenDataset(train_ids, SEQ_LEN)\n",
        "val_ds = NextTokenDataset(val_ids, SEQ_LEN)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMLM(\n",
              "  (emb): Embedding(3655, 128, padding_idx=0)\n",
              "  (rnn): LSTM(128, 64, batch_first=True)\n",
              "  (proj): Linear(in_features=64, out_features=3655, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LSTMLM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb: int=128, hid: int=64, layers: int=1, dropout: float=0.0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=PAD)\n",
        "        self.rnn = nn.LSTM(input_size=emb, hidden_size=hid, num_layers=layers, dropout=dropout if layers>1 else 0.0, batch_first=True)\n",
        "        self.proj = nn.Linear(hid, vocab_size)\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        o,_ = self.rnn(e)\n",
        "        return self.proj(o)\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prefix_ids: List[int], max_new: int=20, eos_id: int=EOS):\n",
        "        self.eval()\n",
        "        x = torch.tensor([prefix_ids], dtype=torch.long, device=DEVICE)\n",
        "        out = x\n",
        "        for _ in range(max_new):\n",
        "            logits = self.forward(out)\n",
        "            nxt = torch.argmax(logits[:,-1,:], dim=-1, keepdim=True)\n",
        "            out = torch.cat([out, nxt], dim=1)\n",
        "            if int(nxt.item()) == eos_id:\n",
        "                break\n",
        "        return out[0].tolist()\n",
        "\n",
        "model = LSTMLM(len(vocab)).to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "crit = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "windows: 100%|██████████| 800/800 [00:00<00:00, 400363.11it/s]\n",
            "windows: 100%|██████████| 100/100 [00:00<00:00, 100007.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples: 800 val samples: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def build_fixed_window_samples(seqs: Sequence[List[int]], seq_len: int) -> List[Tuple[List[int], List[int]]]:\n",
        "    samples: List[Tuple[List[int], List[int]]] = []\n",
        "    for s in tqdm(seqs, desc=\"windows\"):\n",
        "        if not s:\n",
        "            continue\n",
        "        x = s[:seq_len]\n",
        "        y = s[1:seq_len+1]\n",
        "\n",
        "        if len(x) < seq_len:\n",
        "            x = x + [PAD] * (seq_len - len(x))\n",
        "\n",
        "        if len(y) < seq_len:\n",
        "            y = y + [EOS] + [PAD] * max(0, seq_len - len(y) - 1)\n",
        "        samples.append((x, y))\n",
        "    return samples\n",
        "\n",
        "class FixedWindowDataset(Dataset):\n",
        "    def __init__(self, samples: Sequence[Tuple[List[int], List[int]]]):\n",
        "        self.samples = list(samples)\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_ids = [encode(s) for s in pd.read_csv(TRAIN)['text'].astype(str).tolist()]\n",
        "val_ids = [encode(s) for s in pd.read_csv(VAL)['text'].astype(str).tolist()]\n",
        "\n",
        "train_samples = build_fixed_window_samples(train_ids, SEQ_LEN)\n",
        "val_samples = build_fixed_window_samples(val_ids, SEQ_LEN)\n",
        "\n",
        "train_ds = FixedWindowDataset(train_samples)\n",
        "val_ds = FixedWindowDataset(val_samples)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print('train samples:', len(train_ds), 'val samples:', len(val_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/1 - train: 8.2025  val: 8.1779\n",
            "OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os, json\n",
        "from math import inf\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(DEVICE);\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "def train_epoch(dl):\n",
        "    model.train()\n",
        "    total, n = 0.0, 0\n",
        "    for xb, yb in tqdm(dl, total=len(dl), desc=\"train\", leave=False):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += float(loss.item()); n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_loss(dl):\n",
        "    model.eval()\n",
        "    total, n = 0.0, 0\n",
        "    for xb, yb in tqdm(dl, total=len(dl), desc=\"val\", leave=False):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        total += float(loss.item()); n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "best = inf\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = train_epoch(train_dl)\n",
        "    vl = val_loss(val_dl)\n",
        "    print(f\"epoch {epoch}/{EPOCHS} - train: {tr:.4f}  val: {vl:.4f}\")\n",
        "    if vl < best:\n",
        "        best = vl\n",
        "        torch.save(model.state_dict(), 'models/lstm_model.pt')\n",
        "        with open('models/vocab.json','w',encoding='utf-8') as f:\n",
        "            json.dump(stoi, f, ensure_ascii=False)\n",
        "\n",
        "print(\"OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install evaluate rouge_score\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def ids_to_text(ids: List[int]) -> str:\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        if i == EOS: break\n",
        "        toks.append(itos.get(i,'<unk>'))\n",
        "    return ' '.join(toks)\n",
        "\n",
        "preds, refs = [], []\n",
        "val_texts = pd.read_csv(VAL)['text'].astype(str).tolist()\n",
        "for t in tqdm(val_texts, desc=\"eval-lstm\"):\n",
        "    ids = encode(t)\n",
        "    if len(ids) < 4: continue\n",
        "    cut = int(len(ids)*0.75)\n",
        "    prefix, tail = ids[:cut], ids[cut:]\n",
        "    gen = model.generate(prefix, max_new=20, eos_id=EOS)[len(prefix):]\n",
        "    preds.append(ids_to_text(gen))\n",
        "    refs.append(ids_to_text(tail))\n",
        "\n",
        "scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "# Save metrics to file\n",
        "os.makedirs('models', exist_ok=True)\n",
        "with open('models/rouge_lstm.json','w',encoding='utf-8') as f:\n",
        "    json.dump({k: float(v) for k,v in scores.items()}, f, ensure_ascii=False, indent=2)\n",
        "scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install transformers\n",
        "from transformers import pipeline\n",
        "import torch, os\n",
        "\n",
        "# Disable widget-based progress bars globally\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
        "\n",
        "device_index = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Use half precision on GPU to speed up\n",
        "model_kwargs = {'torch_dtype': torch.float16} if torch.cuda.is_available() else {}\n",
        "\n",
        "gen = pipeline('text-generation', model='distilgpt2', device=device_index, model_kwargs=model_kwargs)\n",
        "\n",
        "# Ensure pad_token is set for batching\n",
        "if gen.tokenizer.pad_token_id is None:\n",
        "    gen.tokenizer.pad_token = gen.tokenizer.eos_token\n",
        "if getattr(gen.model.config, 'pad_token_id', None) is None:\n",
        "    gen.model.config.pad_token_id = gen.model.config.eos_token_id\n",
        "\n",
        "# Prepare prefixes/refs once, then run batched generation\n",
        "prefixes, refs_t = [], []\n",
        "for t in val_texts:\n",
        "    ws = t.split()\n",
        "    if len(ws) < 4:\n",
        "        continue\n",
        "    cut = int(len(ws)*0.75)\n",
        "    prefixes.append(' '.join(ws[:cut]))\n",
        "    refs_t.append(' '.join(ws[cut:]))\n",
        "\n",
        "preds_t = []\n",
        "BATCH_INFER = 16 if torch.cuda.is_available() else 4\n",
        "for i in tqdm(range(0, len(prefixes), BATCH_INFER), desc=\"eval-gpt2(batched)\", disable=True):\n",
        "    batch = prefixes[i:i+BATCH_INFER]\n",
        "    outs = gen(batch, max_new_tokens=20, do_sample=True, top_k=50, top_p=0.95, truncation=True, padding=True, batch_size=BATCH_INFER)\n",
        "    # outs is a list of lists (num_inputs x num_return_sequences)\n",
        "    for inp, out in zip(batch, outs):\n",
        "        completion = out[0]['generated_text'][len(inp):].strip()\n",
        "        preds_t.append(completion)\n",
        "\n",
        "scores_t = rouge.compute(predictions=preds_t, references=refs_t, use_stemmer=True)\n",
        "# Save metrics to file\n",
        "with open('models/rouge_distilgpt2.json','w',encoding='utf-8') as f:\n",
        "    json.dump({k: float(v) for k,v in scores_t.items()}, f, ensure_ascii=False, indent=2)\n",
        "scores_t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save and show samples for LSTM and DistilGPT2\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "samples_path = os.path.join('models','generation_samples.txt')\n",
        "os.makedirs('models', exist_ok=True)\n",
        "with open(samples_path, 'w', encoding='utf-8') as fh:\n",
        "    fh.write('Samples LSTM:\\n\\n')\n",
        "    shown = 0\n",
        "    for t in val_texts:\n",
        "        ws = t.split()\n",
        "        if len(ws) < 4: continue\n",
        "        cut = int(len(ws)*0.75)\n",
        "        prefix = ' '.join(ws[:cut])\n",
        "        ref = ' '.join(ws[cut:])\n",
        "        ids = encode(prefix)\n",
        "        gen_tail = model.generate(ids, max_new=20, eos_id=EOS)[len(ids):]\n",
        "        pred = ' '.join([itos.get(i,'<unk>') for i in gen_tail if i!=EOS]).strip()\n",
        "        fh.write(f\"prefix: {prefix}\\nref:    {ref}\\npred:   {pred}\\n\\n\")\n",
        "        print(f\"prefix: {prefix}\\nref:    {ref}\\npred:   {pred}\\n\")\n",
        "        shown += 1\n",
        "        if shown >= 5: break\n",
        "\n",
        "    fh.write('\\nSamples DistilGPT2:\\n\\n')\n",
        "    device_index = 0 if torch.cuda.is_available() else -1\n",
        "    pl = pipeline('text-generation', model='distilgpt2', device=device_index)\n",
        "    shown = 0\n",
        "    for t in val_texts:\n",
        "        ws = t.split()\n",
        "        if len(ws) < 4: continue\n",
        "        cut = int(len(ws)*0.75)\n",
        "        prefix = ' '.join(ws[:cut])\n",
        "        ref = ' '.join(ws[cut:])\n",
        "        out = pl(prefix, max_new_tokens=20, do_sample=True, top_k=50, top_p=0.95, truncation=True)\n",
        "        pred = out[0]['generated_text'][len(prefix):].strip()\n",
        "        fh.write(f\"prefix: {prefix}\\nref:    {ref}\\npred:   {pred}\\n\\n\")\n",
        "        print(f\"prefix: {prefix}\\nref:    {ref}\\npred:   {pred}\\n\")\n",
        "        shown += 1\n",
        "        if shown >= 5: break\n",
        "\n",
        "print(f\"Saved sample comparisons to: {samples_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На валидации простая LSTM на ограниченном сабсэмпле (≈1000 строк) ожидаемо показала низкие ROUGE. При этом предобученная DistilGPT2 стабильно выдаёт более осмысленные продолжения и выигрывает по метрикам. Для итогового использования в задаче автодополнения я выбираю DistilGPT2: качество выше «из коробки», а время инференса на GPU приемлемое.\n",
        "\n",
        "Отдельно поясню выбор размера выборки у меня RTX 3050, и на полном датасете обучение и инференс идут очень медленно; ВМ, которую должны предоставлять, мне не допступна, пытался раз 6-7. Поэтому для проверки пайплайна и сравнения моделей я использовал выборку из 1000 строк этого достаточно для отладки, хотя и мало для высоких метрик.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
