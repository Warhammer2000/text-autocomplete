{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, re, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import List, Dict, Tuple, Sequence\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_DIR = 'data'\n",
        "RAW1 = os.path.join(DATA_DIR, 'raw_dataset.csv')\n",
        "RAW2 = os.path.join(DATA_DIR, 'training.1600000.processed.noemoticon.csv')\n",
        "PROCESSED = os.path.join(DATA_DIR, 'dataset_processed.csv')\n",
        "TRAIN = os.path.join(DATA_DIR, 'train.csv')\n",
        "VAL = os.path.join(DATA_DIR, 'val.csv')\n",
        "TEST = os.path.join(DATA_DIR, 'test.csv')\n",
        "VAL_RATIO = 0.1\n",
        "TEST_RATIO = 0.1\n",
        "MAX_VOCAB = 30000\n",
        "SEQ_LEN = 16\n",
        "BATCH_SIZE = 512\n",
        "LR = 1e-3\n",
        "EPOCHS = 1\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SPECIAL_TOKENS = ['<pad>', '<unk>', '<eos>']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source CSV: data\\training.1600000.processed.noemoticon.csv\n",
            "Processed rows: 1000\n",
            "Splits -> train: 800 val: 100 test: 100\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1000, 800, 100, 100)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Загрузка/чистка и сплиты\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Поиск исходного CSV, если стандартные пути отсутствуют\n",
        "def find_source_csv() -> str | None:\n",
        "    candidates = []\n",
        "    for root, _, files in os.walk(DATA_DIR):\n",
        "        for fn in files:\n",
        "            if not fn.lower().endswith('.csv'):\n",
        "                continue\n",
        "            full = os.path.join(root, fn)\n",
        "            if os.path.basename(full) in {'dataset_processed.csv','train.csv','val.csv','test.csv'}:\n",
        "                continue\n",
        "            candidates.append((full, os.path.getsize(full)))\n",
        "    if not candidates:\n",
        "        return None\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    return candidates[0][0]\n",
        "\n",
        "def nonempty_rows(path: str) -> int:\n",
        "    try:\n",
        "        dfh = pd.read_csv(path, nrows=5)\n",
        "        return int(dfh.shape[0])\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "src_path = None\n",
        "# Предпочитаем Kaggle-файл, если он есть\n",
        "if os.path.exists(RAW2):\n",
        "    src_path = RAW2\n",
        "elif os.path.exists(RAW1) and nonempty_rows(RAW1) > 0:\n",
        "    src_path = RAW1\n",
        "else:\n",
        "    src_path = find_source_csv()\n",
        "\n",
        "if src_path is None:\n",
        "    raise FileNotFoundError('Положите CSV с колонкой text в data/raw_dataset.csv или Kaggle CSV в data/training.1600000.processed.noemoticon.csv')\n",
        "\n",
        "print('Source CSV:', src_path)\n",
        "\n",
        "# Чтение исходного CSV\n",
        "if os.path.basename(src_path).lower().startswith('training.1600000.processed.noemoticon'):\n",
        "    df = pd.read_csv(src_path, encoding='latin-1', header=None)\n",
        "    df = df[[5]].rename(columns={5:'text'})\n",
        "else:\n",
        "    try:\n",
        "        df = pd.read_csv(src_path)\n",
        "        if 'text' not in df.columns:\n",
        "            tmp = pd.read_csv(src_path, header=None, encoding='latin-1')\n",
        "            df = tmp[[5]].rename(columns={5:'text'})\n",
        "    except Exception:\n",
        "        tmp = pd.read_csv(src_path, header=None, encoding='latin-1')\n",
        "        df = tmp[[5]].rename(columns={5:'text'})\n",
        "\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    t = str(t).lower()\n",
        "    t = re.sub(r'https?://\\\\S+|www\\\\.\\\\S+', ' ', t)\n",
        "    t = re.sub(r'@[\\\\w_]+', ' ', t)\n",
        "    t = re.sub(r'\\\\s+', ' ', t).strip()\n",
        "    return t\n",
        "\n",
        "df['text'] = df['text'].map(clean_text)\n",
        "df = df.dropna().drop_duplicates().reset_index(drop=True)\n",
        "mask = df['text'].astype(str).str.len() > 0\n",
        "df = df[mask].reset_index(drop=True)\n",
        "\n",
        "\n",
        "MAX_ROWS = 1000\n",
        "if MAX_ROWS:\n",
        "    df = df.iloc[:MAX_ROWS]\n",
        "\n",
        "\n",
        "df.to_csv(PROCESSED, index=False, encoding='utf-8')\n",
        "print('Processed rows:', len(df))\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "idx = rng.permutation(len(df))\n",
        "n = len(df)\n",
        "n_test = int(n * TEST_RATIO)\n",
        "n_val = int(n * VAL_RATIO)\n",
        "n_train = max(0, n - n_val - n_test)\n",
        "train_idx = idx[:n_train]\n",
        "val_idx = idx[n_train:n_train+n_val]\n",
        "test_idx = idx[n_train+n_val:]\n",
        "\n",
        "pd.DataFrame({'text': df.iloc[train_idx]['text']}).to_csv(TRAIN, index=False, encoding='utf-8')\n",
        "pd.DataFrame({'text': df.iloc[val_idx]['text']}).to_csv(VAL, index=False, encoding='utf-8')\n",
        "pd.DataFrame({'text': df.iloc[test_idx]['text']}).to_csv(TEST, index=False, encoding='utf-8')\n",
        "\n",
        "print('Splits ->', 'train:', len(train_idx), 'val:', len(val_idx), 'test:', len(test_idx))\n",
        "len(df), len(train_idx), len(val_idx), len(test_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vocab:   0%|          | 0/800 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "vocab: 100%|██████████| 800/800 [00:00<00:00, 177096.28it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(3655, ['<pad>', '<unk>', '<eos>', 'i', 'to', 'the', 'my', 'a', 'and', 'is'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def tokenize(s: str) -> List[str]:\n",
        "    return s.split()\n",
        "\n",
        "\n",
        "texts = pd.read_csv(TRAIN)['text'].astype(str).tolist()\n",
        "cnt = Counter()\n",
        "for t in tqdm(texts, desc=\"vocab\"):\n",
        "    cnt.update(tokenize(t))\n",
        "\n",
        "vocab = SPECIAL_TOKENS + [w for w,_ in cnt.most_common(MAX_VOCAB) if w not in SPECIAL_TOKENS]\n",
        "vocab = vocab[:MAX_VOCAB]\n",
        "stoi: Dict[str,int] = {w:i for i,w in enumerate(vocab)}\n",
        "itos: Dict[int,str] = {i:w for w,i in stoi.items()}\n",
        "PAD, UNK, EOS = stoi['<pad>'], stoi['<unk>'], stoi['<eos>']\n",
        "\n",
        "def encode(s: str, add_eos: bool=True) -> List[int]:\n",
        "    ids = [stoi.get(t, UNK) for t in tokenize(s)]\n",
        "    if add_eos:\n",
        "        ids.append(EOS)\n",
        "    return ids\n",
        "\n",
        "len(vocab), list(stoi)[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1856, 236)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class NextTokenDataset(Dataset):\n",
        "    def __init__(self, sequences: Sequence[List[int]], seq_len: int):\n",
        "        self.samples: List[Tuple[List[int], List[int]]] = []\n",
        "        for s in sequences:\n",
        "            if len(s) < 2: continue\n",
        "            for st in range(0, max(0, len(s)-1)):\n",
        "                ed = st + seq_len\n",
        "                x = s[st:ed]\n",
        "                y = s[st+1:ed+1]\n",
        "                if len(x) < seq_len or len(y) < seq_len: break\n",
        "                self.samples.append((x,y))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x,y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    return torch.stack(xs,0), torch.stack(ys,0)\n",
        "\n",
        "train_ids = [encode(s) for s in pd.read_csv(TRAIN)['text'].astype(str).tolist()]\n",
        "val_ids = [encode(s) for s in pd.read_csv(VAL)['text'].astype(str).tolist()]\n",
        "\n",
        "train_ds = NextTokenDataset(train_ids, SEQ_LEN)\n",
        "val_ds = NextTokenDataset(val_ids, SEQ_LEN)\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMLM(\n",
              "  (emb): Embedding(3655, 128, padding_idx=0)\n",
              "  (rnn): LSTM(128, 64, batch_first=True)\n",
              "  (proj): Linear(in_features=64, out_features=3655, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LSTMLM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, emb: int=128, hid: int=64, layers: int=1, dropout: float=0.0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb, padding_idx=PAD)\n",
        "        self.rnn = nn.LSTM(input_size=emb, hidden_size=hid, num_layers=layers, dropout=dropout if layers>1 else 0.0, batch_first=True)\n",
        "        self.proj = nn.Linear(hid, vocab_size)\n",
        "    def forward(self, x):\n",
        "        e = self.emb(x)\n",
        "        o,_ = self.rnn(e)\n",
        "        return self.proj(o)\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prefix_ids: List[int], max_new: int=20, eos_id: int=EOS):\n",
        "        self.eval()\n",
        "        x = torch.tensor([prefix_ids], dtype=torch.long, device=DEVICE)\n",
        "        out = x\n",
        "        for _ in range(max_new):\n",
        "            logits = self.forward(out)\n",
        "            nxt = torch.argmax(logits[:,-1,:], dim=-1, keepdim=True)\n",
        "            out = torch.cat([out, nxt], dim=1)\n",
        "            if int(nxt.item()) == eos_id:\n",
        "                break\n",
        "        return out[0].tolist()\n",
        "\n",
        "model = LSTMLM(len(vocab)).to(DEVICE)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "crit = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "windows: 100%|██████████| 800/800 [00:00<00:00, 400363.11it/s]\n",
            "windows: 100%|██████████| 100/100 [00:00<00:00, 100007.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train samples: 800 val samples: 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def build_fixed_window_samples(seqs: Sequence[List[int]], seq_len: int) -> List[Tuple[List[int], List[int]]]:\n",
        "    samples: List[Tuple[List[int], List[int]]] = []\n",
        "    for s in tqdm(seqs, desc=\"windows\"):\n",
        "        if not s:\n",
        "            continue\n",
        "        x = s[:seq_len]\n",
        "        y = s[1:seq_len+1]\n",
        "\n",
        "        if len(x) < seq_len:\n",
        "            x = x + [PAD] * (seq_len - len(x))\n",
        "\n",
        "        if len(y) < seq_len:\n",
        "            y = y + [EOS] + [PAD] * max(0, seq_len - len(y) - 1)\n",
        "        samples.append((x, y))\n",
        "    return samples\n",
        "\n",
        "class FixedWindowDataset(Dataset):\n",
        "    def __init__(self, samples: Sequence[Tuple[List[int], List[int]]]):\n",
        "        self.samples = list(samples)\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.samples[idx]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "train_ids = [encode(s) for s in pd.read_csv(TRAIN)['text'].astype(str).tolist()]\n",
        "val_ids = [encode(s) for s in pd.read_csv(VAL)['text'].astype(str).tolist()]\n",
        "\n",
        "train_samples = build_fixed_window_samples(train_ids, SEQ_LEN)\n",
        "val_samples = build_fixed_window_samples(val_ids, SEQ_LEN)\n",
        "\n",
        "train_ds = FixedWindowDataset(train_samples)\n",
        "val_ds = FixedWindowDataset(val_samples)\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
        "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "print('train samples:', len(train_ds), 'val samples:', len(val_ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "2.6.0+cu124\n",
            "12.4\n",
            "True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "train:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1/1 - train: 8.2025  val: 8.1779\n",
            "OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os, json\n",
        "from math import inf\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(DEVICE);\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "def train_epoch(dl):\n",
        "    model.train()\n",
        "    total, n = 0.0, 0\n",
        "    for xb, yb in tqdm(dl, total=len(dl), desc=\"train\", leave=False):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total += float(loss.item()); n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "@torch.no_grad()\n",
        "def val_loss(dl):\n",
        "    model.eval()\n",
        "    total, n = 0.0, 0\n",
        "    for xb, yb in tqdm(dl, total=len(dl), desc=\"val\", leave=False):\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        logits = model(xb)\n",
        "        loss = crit(logits.view(-1, logits.size(-1)), yb.view(-1))\n",
        "        total += float(loss.item()); n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "best = inf\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = train_epoch(train_dl)\n",
        "    vl = val_loss(val_dl)\n",
        "    print(f\"epoch {epoch}/{EPOCHS} - train: {tr:.4f}  val: {vl:.4f}\")\n",
        "    if vl < best:\n",
        "        best = vl\n",
        "        torch.save(model.state_dict(), 'models/lstm_model.pt')\n",
        "        with open('models/vocab.json','w',encoding='utf-8') as f:\n",
        "            json.dump(stoi, f, ensure_ascii=False)\n",
        "\n",
        "print(\"OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "eval-lstm: 100%|██████████| 100/100 [00:01<00:00, 63.84it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': np.float64(0.003945197294950989),\n",
              " 'rouge2': np.float64(0.0),\n",
              " 'rougeL': np.float64(0.003916125837308103),\n",
              " 'rougeLsum': np.float64(0.00394519729495099)}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip -q install evaluate rouge_score\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "def ids_to_text(ids: List[int]) -> str:\n",
        "    toks = []\n",
        "    for i in ids:\n",
        "        if i == EOS: break\n",
        "        toks.append(itos.get(i,'<unk>'))\n",
        "    return ' '.join(toks)\n",
        "\n",
        "preds, refs = [], []\n",
        "val_texts = pd.read_csv(VAL)['text'].astype(str).tolist()\n",
        "for t in tqdm(val_texts[:1000], desc=\"eval-lstm\"):\n",
        "    ids = encode(t)\n",
        "    if len(ids) < 4: continue\n",
        "    cut = int(len(ids)*0.75)\n",
        "    prefix, tail = ids[:cut], ids[cut:]\n",
        "    gen = model.generate(prefix, max_new=20, eos_id=EOS)[len(prefix):]\n",
        "    preds.append(ids_to_text(gen))\n",
        "    refs.append(ids_to_text(tail))\n",
        "\n",
        "scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
        "scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "Device set to use cuda:0\n",
            "eval-gpt2:   0%|          | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   1%|          | 1/100 [00:03<06:21,  3.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   2%|▏         | 2/100 [00:06<05:33,  3.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   4%|▍         | 4/100 [00:10<03:41,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   5%|▌         | 5/100 [00:11<03:19,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   6%|▌         | 6/100 [00:15<03:53,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   7%|▋         | 7/100 [00:18<04:07,  2.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   8%|▊         | 8/100 [00:18<02:55,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  11%|█         | 11/100 [00:19<01:22,  1.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  12%|█▏        | 12/100 [00:23<02:20,  1.59s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  13%|█▎        | 13/100 [00:26<02:45,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=34) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  14%|█▍        | 14/100 [00:28<03:00,  2.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  16%|█▌        | 16/100 [00:31<02:39,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  17%|█▋        | 17/100 [00:32<02:08,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  18%|█▊        | 18/100 [00:34<02:30,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  19%|█▉        | 19/100 [00:37<02:48,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  20%|██        | 20/100 [00:41<03:12,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  21%|██        | 21/100 [00:43<03:16,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  22%|██▏       | 22/100 [00:46<03:22,  2.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  23%|██▎       | 23/100 [00:49<03:25,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  24%|██▍       | 24/100 [00:52<03:37,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=33) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  25%|██▌       | 25/100 [00:53<02:36,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  26%|██▌       | 26/100 [00:55<02:50,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  27%|██▋       | 27/100 [00:58<02:54,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  28%|██▊       | 28/100 [01:01<03:00,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  29%|██▉       | 29/100 [01:02<02:33,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  30%|███       | 30/100 [01:05<02:48,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  31%|███       | 31/100 [01:08<02:52,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  32%|███▏      | 32/100 [01:08<02:01,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  33%|███▎      | 33/100 [01:11<02:17,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  34%|███▍      | 34/100 [01:14<02:43,  2.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=34) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  35%|███▌      | 35/100 [01:17<02:50,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  36%|███▌      | 36/100 [01:20<02:57,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  37%|███▋      | 37/100 [01:23<03:02,  2.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  38%|███▊      | 38/100 [01:23<02:08,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  39%|███▉      | 39/100 [01:26<02:17,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  40%|████      | 40/100 [01:29<02:23,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  41%|████      | 41/100 [01:32<02:26,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  42%|████▏     | 42/100 [01:35<02:33,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  43%|████▎     | 43/100 [01:35<01:49,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  44%|████▍     | 44/100 [01:37<02:00,  2.15s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=34) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  45%|████▌     | 45/100 [01:38<01:26,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  46%|████▌     | 46/100 [01:40<01:40,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  47%|████▋     | 47/100 [01:43<01:50,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  48%|████▊     | 48/100 [01:46<02:01,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  49%|████▉     | 49/100 [01:46<01:28,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  50%|█████     | 50/100 [01:49<01:40,  2.01s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=36) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  51%|█████     | 51/100 [01:52<01:49,  2.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  53%|█████▎    | 53/100 [01:54<01:26,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  55%|█████▌    | 55/100 [01:57<01:16,  1.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  56%|█████▌    | 56/100 [02:00<01:23,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  57%|█████▋    | 57/100 [02:03<01:29,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  58%|█████▊    | 58/100 [02:03<01:07,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  59%|█████▉    | 59/100 [02:06<01:22,  2.02s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  60%|██████    | 60/100 [02:08<01:26,  2.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  61%|██████    | 61/100 [02:11<01:29,  2.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  62%|██████▏   | 62/100 [02:14<01:32,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  63%|██████▎   | 63/100 [02:17<01:39,  2.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  64%|██████▍   | 64/100 [02:18<01:13,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  65%|██████▌   | 65/100 [02:20<01:18,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  68%|██████▊   | 68/100 [02:21<00:34,  1.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  69%|██████▉   | 69/100 [02:21<00:28,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  70%|███████   | 70/100 [02:24<00:41,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  71%|███████   | 71/100 [02:27<00:51,  1.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  72%|███████▏  | 72/100 [02:30<00:57,  2.05s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  73%|███████▎  | 73/100 [02:31<00:47,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  74%|███████▍  | 74/100 [02:34<00:54,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  75%|███████▌  | 75/100 [02:37<00:57,  2.28s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  77%|███████▋  | 77/100 [02:40<00:44,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  78%|███████▊  | 78/100 [02:42<00:46,  2.12s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  79%|███████▉  | 79/100 [02:45<00:47,  2.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  80%|████████  | 80/100 [02:46<00:41,  2.08s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  81%|████████  | 81/100 [02:49<00:43,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  82%|████████▏ | 82/100 [02:52<00:45,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  83%|████████▎ | 83/100 [02:53<00:32,  1.91s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  84%|████████▍ | 84/100 [02:56<00:35,  2.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  86%|████████▌ | 86/100 [02:58<00:25,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  87%|████████▋ | 87/100 [02:59<00:18,  1.39s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  89%|████████▉ | 89/100 [03:01<00:15,  1.40s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  90%|█████████ | 90/100 [03:04<00:17,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  91%|█████████ | 91/100 [03:07<00:17,  1.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  92%|█████████▏| 92/100 [03:09<00:16,  2.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  93%|█████████▎| 93/100 [03:12<00:16,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  94%|█████████▍| 94/100 [03:15<00:14,  2.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  95%|█████████▌| 95/100 [03:18<00:12,  2.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  96%|█████████▌| 96/100 [03:20<00:10,  2.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=33) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  97%|█████████▋| 97/100 [03:21<00:05,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  98%|█████████▊| 98/100 [03:24<00:04,  2.25s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  99%|█████████▉| 99/100 [03:26<00:02,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2: 100%|██████████| 100/100 [03:29<00:00,  2.10s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': np.float64(0.02971537197311072),\n",
              " 'rouge2': np.float64(0.0017211784170904241),\n",
              " 'rougeL': np.float64(0.028309593153499332),\n",
              " 'rougeLsum': np.float64(0.028711034495299652)}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip -q install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "gen = pipeline('text-generation', model='distilgpt2')\n",
        "\n",
        "preds_t, refs_t = [], []\n",
        "for t in tqdm(val_texts[:200], desc=\"eval-gpt2\"):\n",
        "    ws = t.split()\n",
        "    if len(ws) < 4: continue\n",
        "    cut = int(len(ws)*0.75)\n",
        "    prefix = ' '.join(ws[:cut])\n",
        "    ref = ' '.join(ws[cut:])\n",
        "    out = gen(prefix, max_length=cut+20, do_sample=True, top_k=50, top_p=0.95)\n",
        "    completion = out[0]['generated_text'][len(prefix):].strip()\n",
        "    preds_t.append(completion)\n",
        "    refs_t.append(ref)\n",
        "\n",
        "scores_t = rouge.compute(predictions=preds_t, references=refs_t, use_stemmer=True)\n",
        "scores_t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Samples LSTM:\\n\")\n",
        "shown = 0\n",
        "for t in val_texts[:10]:\n",
        "    ws = t.split()\n",
        "    if len(ws) < 4: continue\n",
        "    cut = int(len(ws)*0.75)\n",
        "    prefix = ' '.join(ws[:cut])\n",
        "    ref = ' '.join(ws[cut:])\n",
        "    ids = encode(prefix)\n",
        "    gen_tail = model.generate(ids, max_new=20, eos_id=EOS)[len(ids):]\n",
        "    pred = ' '.join([itos.get(i,'<unk>') for i in gen_tail if i!=EOS]).strip()\n",
        "    print(f\"prefix: {prefix}\\nref:    {ref}\\npred:   {pred}\\n\")\n",
        "    shown += 1\n",
        "    if shown >= 5: break\n",
        "\n",
        "print(\"\\nSamples DistilGPT2:\\n\")\n",
        "from transformers import pipeline\n",
        "pl = pipeline('text-generation', model='distilgpt2')\n",
        "shown = 0\n",
        "for t in val_texts[:10]:\n",
        "    ws = t.split()\n",
        "    if len(ws) < 4: continue\n",
        "    cut = int(len(ws)*0.75)\n",
        "    prefix = ' '.join(ws[:cut])\n",
        "    ref = ' '.join(ws[cut:])\n",
        "    out = pl(prefix, max_length=cut+20, do_sample=True, top_k=50, top_p=0.95)\n",
        "    pred = out[0]['generated_text'][len(prefix):].strip()\n",
        "    print(f\"prefix: {prefix}\\nref:    {ref}\\npred:   {pred}\\n\")\n",
        "    shown += 1\n",
        "    if shown >= 5: break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На валидации простая LSTM на ограниченном сабсэмпле (≈1000 строк) ожидаемо показала низкие ROUGE. При этом предобученная DistilGPT2 стабильно выдаёт более осмысленные продолжения и выигрывает по метрикам. Для итогового использования в задаче автодополнения я выбираю DistilGPT2: качество выше «из коробки», а время инференса на GPU приемлемое.\n",
        "\n",
        "Отдельно поясню выбор размера выборки у меня RTX 3050, и на полном датасете обучение и инференс идут очень медленно; ВМ, которую должны предоставлять, мне не допступна, пытался раз 6-7. Поэтому для проверки пайплайна и сравнения моделей я использовал выборку из 1000 строк этого достаточно для отладки, хотя и мало для высоких метрик.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
