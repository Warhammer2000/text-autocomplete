{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
            "CWD: c:\\Users\\User\\Desktop\\text-autocomplete\n",
            "Конфиг загружен: {'raw_path': 'data/raw_dataset.csv', 'processed_path': 'data/dataset_processed.csv', 'train_path': 'data/train.csv', 'val_path': 'data/val.csv', 'test_path': 'data/test.csv', 'val_ratio': 0.1, 'test_ratio': 0.1, 'max_rows': 1000}\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"CWD: {os.getcwd()}\")\n",
        "\n",
        "with open(\"configs/default.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "print(\"Конфиг загружен:\", cfg[\"data\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already prepared. Skipping.\n",
            "OK: data prepared via src.prepare_data with MAX_ROWS=1000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from importlib import import_module\n",
        "\n",
        "os.environ[\"MAX_ROWS\"] = \"1000\"\n",
        "\n",
        "prepare = import_module(\"src.prepare_data\")\n",
        "prepare.main()\n",
        "\n",
        "print(\"OK: data prepared via src.prepare_data with MAX_ROWS=1000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print('CUDA available:', torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed: data/dataset_processed.csv -> 1000 rows\n",
            "train: data/train.csv -> 800 rows\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@marcusmims wow i didn't get an &amp;quot;hello&amp;qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@majesticflame ouch - sounds very sucky</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@reannaremick doesnt work on my cell  go to sl...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  @marcusmims wow i didn't get an &quot;hello&qu...\n",
              "1            @majesticflame ouch - sounds very sucky\n",
              "2  @reannaremick doesnt work on my cell  go to sl..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val: data/val.csv -> 100 rows\n",
            "test: data/test.csv -> 100 rows\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import yaml\n",
        "\n",
        "with open('configs/default.yaml','r',encoding='utf-8') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "paths = {\n",
        "    'processed': cfg['data']['processed_path'],\n",
        "    'train': cfg['data']['train_path'],\n",
        "    'val': cfg['data']['val_path'],\n",
        "    'test': cfg['data']['test_path'],\n",
        "}\n",
        "\n",
        "for name, path in paths.items():\n",
        "    try:\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"{name}: {path} -> {len(df)} rows\")\n",
        "        if name == 'train':\n",
        "            display(df.head(3))\n",
        "    except Exception as e:\n",
        "        print(f\"{name}: MISSING -> {path} ({e})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\Desktop\\text-autocomplete\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01/5 | train: 8.1141 | val: 8.0741 | ROUGE1: 0.0042 | ROUGE2: 0.0007\n",
            "> prefix: sad that the 'feet' of my macbook just fell off : sad that the 'feet' of\n",
            "> ref:    my macbook just fell off\n",
            "> pred:   day day day day lonely in in than does season her autograph bah day day in thing does next next\n",
            "> prefix: i wanna <unk> into the <unk> and play with\n",
            "> ref:    the <unk>\n",
            "> pred:   bathroom... go go was! was! tropic tropic &quot;i'm &quot;i'm &quot;i'm stuffing than &quot;i'm than &quot;i'm &quot;i'm &quot;i'm some1 &quot;i'm &quot;i'm\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 02/5 | train: 8.0523 | val: 7.9991 | ROUGE1: 0.0061 | ROUGE2: 0.0000\n",
            "> prefix: sad that the 'feet' of my macbook just fell off : sad that the 'feet' of\n",
            "> ref:    my macbook just fell off\n",
            "> pred:   \n",
            "> prefix: i wanna <unk> into the <unk> and play with\n",
            "> ref:    the <unk>\n",
            "> pred:   with again again got\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 03/5 | train: 7.9217 | val: 7.7945 | ROUGE1: 0.0000 | ROUGE2: 0.0000\n",
            "> prefix: sad that the 'feet' of my macbook just fell off : sad that the 'feet' of\n",
            "> ref:    my macbook just fell off\n",
            "> pred:   \n",
            "> prefix: i wanna <unk> into the <unk> and play with\n",
            "> ref:    the <unk>\n",
            "> pred:   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 04/5 | train: 7.5740 | val: 7.3437 | ROUGE1: 0.0000 | ROUGE2: 0.0000\n",
            "> prefix: sad that the 'feet' of my macbook just fell off : sad that the 'feet' of\n",
            "> ref:    my macbook just fell off\n",
            "> pred:   \n",
            "> prefix: i wanna <unk> into the <unk> and play with\n",
            "> ref:    the <unk>\n",
            "> pred:   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                    \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 05/5 | train: 7.0648 | val: 6.9150 | ROUGE1: 0.0000 | ROUGE2: 0.0000\n",
            "> prefix: sad that the 'feet' of my macbook just fell off : sad that the 'feet' of\n",
            "> ref:    my macbook just fell off\n",
            "> pred:   \n",
            "> prefix: i wanna <unk> into the <unk> and play with\n",
            "> ref:    the <unk>\n",
            "> pred:   \n",
            "Saved model to models\\lstm_model.pt\n",
            "Saved vocab to models\\vocab.json\n",
            "Saved config to models\\lstm_config.json\n",
            "\n",
            "Final evaluation (test, ROUGE):\n",
            "{'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
          ]
        }
      ],
      "source": [
        "from importlib import import_module\n",
        "train = import_module(\"src.lstm_train\")\n",
        "train.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE: {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0}\n"
          ]
        }
      ],
      "source": [
        "from importlib import import_module\n",
        "eval_lstm = import_module(\"src.eval_lstm\")\n",
        "eval_lstm.main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "eval-gpt2:   0%|          | 0/100 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   1%|          | 1/100 [00:04<07:32,  4.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   2%|▏         | 2/100 [00:07<06:13,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   4%|▍         | 4/100 [00:11<03:53,  2.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   5%|▌         | 5/100 [00:14<04:12,  2.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   6%|▌         | 6/100 [00:15<03:20,  2.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   7%|▋         | 7/100 [00:18<03:53,  2.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   8%|▊         | 8/100 [00:22<04:19,  2.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:   9%|▉         | 9/100 [00:25<04:38,  3.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  11%|█         | 11/100 [00:28<03:17,  2.22s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  12%|█▏        | 12/100 [00:31<03:31,  2.40s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  13%|█▎        | 13/100 [00:34<03:44,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=34) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  14%|█▍        | 14/100 [00:37<03:49,  2.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  15%|█▌        | 15/100 [00:40<04:02,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  16%|█▌        | 16/100 [00:43<03:55,  2.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  17%|█▋        | 17/100 [00:46<03:55,  2.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  18%|█▊        | 18/100 [00:48<03:47,  2.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  19%|█▉        | 19/100 [00:51<03:43,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  20%|██        | 20/100 [00:54<03:40,  2.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  21%|██        | 21/100 [00:57<03:45,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  22%|██▏       | 22/100 [00:59<03:39,  2.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  23%|██▎       | 23/100 [01:00<02:34,  2.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  24%|██▍       | 24/100 [01:01<02:18,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=33) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  25%|██▌       | 25/100 [01:02<01:58,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  27%|██▋       | 27/100 [01:04<01:32,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  29%|██▉       | 29/100 [01:04<00:57,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  30%|███       | 30/100 [01:07<01:30,  1.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  33%|███▎      | 33/100 [01:10<01:11,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  34%|███▍      | 34/100 [01:12<01:28,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=34) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  35%|███▌      | 35/100 [01:14<01:42,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  36%|███▌      | 36/100 [01:15<01:21,  1.27s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  37%|███▋      | 37/100 [01:19<01:59,  1.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  38%|███▊      | 38/100 [01:24<02:57,  2.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  39%|███▉      | 39/100 [01:30<03:39,  3.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  40%|████      | 40/100 [01:35<04:02,  4.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  41%|████      | 41/100 [01:40<04:23,  4.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  42%|████▏     | 42/100 [01:46<04:31,  4.68s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  43%|████▎     | 43/100 [01:51<04:32,  4.77s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  44%|████▍     | 44/100 [01:56<04:39,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=34) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  45%|████▌     | 45/100 [01:57<03:34,  3.90s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  46%|████▌     | 46/100 [02:03<03:54,  4.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  47%|████▋     | 47/100 [02:08<04:06,  4.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  48%|████▊     | 48/100 [02:09<03:04,  3.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  49%|████▉     | 49/100 [02:15<03:35,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  50%|█████     | 50/100 [02:20<03:48,  4.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=36) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  51%|█████     | 51/100 [02:23<03:10,  3.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  52%|█████▏    | 52/100 [02:30<03:51,  4.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  53%|█████▎    | 53/100 [02:36<04:04,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  55%|█████▌    | 55/100 [02:41<03:06,  4.14s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  56%|█████▌    | 56/100 [02:48<03:26,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  58%|█████▊    | 58/100 [02:48<01:57,  2.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  59%|█████▉    | 59/100 [02:54<02:20,  3.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  60%|██████    | 60/100 [02:59<02:32,  3.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  61%|██████    | 61/100 [02:59<01:53,  2.92s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  62%|██████▏   | 62/100 [03:05<02:21,  3.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  63%|██████▎   | 63/100 [03:08<02:11,  3.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  64%|██████▍   | 64/100 [03:08<01:32,  2.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  65%|██████▌   | 65/100 [03:13<01:58,  3.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  67%|██████▋   | 67/100 [03:20<01:46,  3.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  68%|██████▊   | 68/100 [03:20<01:20,  2.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  69%|██████▉   | 69/100 [03:20<01:00,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  70%|███████   | 70/100 [03:20<00:43,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=41) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  71%|███████   | 71/100 [03:27<01:27,  3.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  72%|███████▏  | 72/100 [03:33<01:43,  3.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  73%|███████▎  | 73/100 [03:33<01:16,  2.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  74%|███████▍  | 74/100 [03:39<01:31,  3.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  75%|███████▌  | 75/100 [03:44<01:39,  3.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=23) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  76%|███████▌  | 76/100 [03:44<01:08,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  77%|███████▋  | 77/100 [03:50<01:26,  3.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  78%|███████▊  | 78/100 [03:55<01:31,  4.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  79%|███████▉  | 79/100 [04:00<01:35,  4.54s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  80%|████████  | 80/100 [04:05<01:33,  4.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  81%|████████  | 81/100 [04:06<01:06,  3.52s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=32) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  82%|████████▏ | 82/100 [04:11<01:12,  4.03s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  83%|████████▎ | 83/100 [04:12<00:50,  3.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  84%|████████▍ | 84/100 [04:17<00:59,  3.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=37) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  85%|████████▌ | 85/100 [04:23<01:03,  4.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  86%|████████▌ | 86/100 [04:28<01:04,  4.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  87%|████████▋ | 87/100 [04:28<00:42,  3.26s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  88%|████████▊ | 88/100 [04:34<00:46,  3.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  89%|████████▉ | 89/100 [04:35<00:34,  3.10s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  90%|█████████ | 90/100 [04:39<00:33,  3.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  91%|█████████ | 91/100 [04:40<00:23,  2.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  92%|█████████▏| 92/100 [04:45<00:27,  3.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  93%|█████████▎| 93/100 [04:51<00:28,  4.00s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=31) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  94%|█████████▍| 94/100 [04:51<00:17,  2.85s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=26) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  95%|█████████▌| 95/100 [04:55<00:17,  3.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  96%|█████████▌| 96/100 [05:00<00:15,  3.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=33) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  97%|█████████▋| 97/100 [05:05<00:12,  4.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  98%|█████████▊| 98/100 [05:10<00:08,  4.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=29) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "eval-gpt2:  99%|█████████▉| 99/100 [05:15<00:04,  4.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=35) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformer ROUGE: {'rouge1': 0.014087414140878461, 'rouge2': 0.0007370510705865468, 'rougeL': 0.012982953977906744, 'rougeLsum': 0.012815745787937476}\n",
            "\n",
            "Samples (distilgpt2):\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=28) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> prefix: sad that the 'feet' of my macbook just fell off : sad that the 'feet'\n",
            "> ref:    of my macbook just fell off\n",
            "> pred:   of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook just fell off : sad that the 'feet' of my macbook\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=38) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> prefix: i wanna sneak into the zoo and play\n",
            "> ref:    with the kitties\n",
            "> pred:   in the game.\n",
            "\n",
            "\n",
            "This article is about the game. For the game, see the game. For the game, see the game. For the game, see the game. For the game, see the game.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=39) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "> prefix: @ridley1013 i agree. the shapeshifting is a copout. i was so excited for angela's ep, i thought it\n",
            "> ref:    was this week. noah was awesome tho!\n",
            "> pred:   was a good idea to go out and show her how to use the shapeshifting to alter the appearance of the shapeshifters. i did it to her, but it didn't work. the shapeshifters are a pretty simple and fun way to use them.\n",
            "\n",
            "I was on my way to the airport. I heard about it and I thought I'd see the Shapeshifters on the train, but I'm still not sure what to do with it. the shapeshifters are the only thing that actually help them. I'm not sure how I'll get there, but I'm sure I'll have a chance to make some pretty nice friends. the shapeshifters are the only thing that actually help them. I'm not sure how I'll get there, but I'm sure I'll have a chance to make some pretty nice friends. The shapeshifters are the only thing that actually help them. I'm not sure how I'll get there, but I'm sure I'll have a chance to make some pretty nice friends. the shapeshifters are the only thing that actually help them. I'm not sure how I'll get there, but I'm sure I'll have a chance to make some pretty nice\n",
            "\n",
            "> prefix: pray for me please, the ex is threatening to start sh** at my/our babies 1st birthday party. what a\n",
            "> ref:    jerk. and i still have a headache\n",
            "> pred:   terrible decision. I just felt sorry for my parents for being a little too scared to tell my kids they are going to see me.\n",
            "I have been told my parents are going to tell me that I am going to be on the wrong side of the world for my babies. I was told by a young man that I am going to be the only person who could get me there to be able to get back to my birthright. I'm a very well educated, well-meaning, well-meaning person who is able to be very good at this and that's why I am so upset about this.\n",
            "I am going to be able to get to the point where I can go home without any money or anything. I have no money for this trip and I will be able to get to the point where I can go home without any money or anything. I have no money for this trip and I will be able to get to the point where I can go home without any money or anything. I have no money for this trip and I will be able to get to the point where I can go home without any money or anything. I have no money for this trip and I will be able to get to the point where I can go home without any money or\n"
          ]
        }
      ],
      "source": [
        "from importlib import import_module\n",
        "eval_tf = import_module(\"src.eval_transformer_pipeline\")\n",
        "eval_tf.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "На валидации простая LSTM на ограниченном сабсэмпле (≈1000 строк) ожидаемо показала низкие ROUGE. При этом предобученная DistilGPT2 стабильно выдаёт более осмысленные продолжения и выигрывает по метрикам. Для итогового использования в задаче автодополнения я выбираю DistilGPT2: качество выше «из коробки», а время инференса на GPU приемлемое.\n",
        "\n",
        "Отдельно поясню выбор размера выборки у меня RTX 3050, и на полном датасете обучение и инференс идут очень медленно; ВМ, которую должны предоставлять, мне не допступна, пытался раз 6-7. Поэтому для проверки пайплайна и сравнения моделей я использовал выборку из 1000 строк этого достаточно для отладки, хотя и мало для высоких метрик."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
